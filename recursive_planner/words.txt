What's wrong 
So this is the hardest part for sure 

What could be going wrong
- it learns some set action sequence and then gets stuck with it
- literally something buggy, not sure why num_moves is so high!!!
- we're not logging a bunch of important stuff
   - get the logger up and running

What is really going on...
- poss this turn is the same as num_moves < 11...
   - no this is short term bad thinking, poss this turn is poss this turn

Okay so what the f am I really doing here

- Early stopping for the gradient optimizaiton steps so it's all faster?

TODO
- how about we do training differntly than I thought...
- things we can do direcct from buffer we do
   - guess aciton siequence, rew, num_moves
- train gen_poss and poss this turn directly from the plans and actions
- train vision directly from buffer too...



DONE
fix up memories
- What are the memories really

   - actual reward
   - actions taken (sometimes)
   - gen_poss
   - poss_this_turn
   - obs
   - goal (final frame or imagined?)
   - number of steps taken
   - midpoint
- we can get all of this from the experiene buffer if we just have a start frame...
- Acts can then be yes


set up the ability to not learn some examples
- have a dict to tell you what need to be replaced
- create a tensor the size of the thing, with true where it needs to be replaced - broadcasting? learn...
   - max acts to get label
- torch.where it...
refactor to pyrtoch lightning? - clean up stuff...
- i hate how it is right now, but it is working and wont take long to get early stopping...
- would include turning our things into pytorch Datasets
   - could just have our things turn themselves into pytorch datasets when it's time to train?
   - not needing to colate would really clean up the code...
have it not jacobian through the vision network.
- done
find out why it's not learning actions
- loss goes down with lstm style network
endless recursion handling
- if it gets to depth of 8 for each thing
   - max action len 10 (probably increase this to 20 anyways...)
   - 2^8 * 10 = 2560 total actions in one plan...
   - I think we can just set the max depth at 8...

its not training because... view != permute...
it's guessing 0.5 loss for eveythng
- action inputs are vague (multple classes to one acton, how could you learn it)
   - 7 long, 6 is eos token
- my lstm thing literally doesn't work (wouldn't be suprised lol :( ))
   - seems to not just guess 1 thing with random stuff.
- vision is training while everything else is training? and vision make the ins too?
   - freeze vision for 2nd half - nothin...
- batch size too small...
   - nope now it's perfectly pinned to .5 lol
- all the relus are too harsh 
   -  sigmoid?
learning rate was maybe too high?
LEARNING RATE WAS TOO high
1E-4 WORKS, 1E-3 LEARNS MEANS...

working my way up to learning more of the outputs.
it still doesn't seem to learn the probabilities properly
- lower learning rate?
   - no now it's just worse everywehre...
- more epochs?
- too small of batch size?
- too small of batch size and too big of learning rate?

making plans shorter for more dense data
not really learning unfortulatly

so rew can be guessed for long sequences, but not short ones
num_moves cant be guessed in long sequences, doesnt work 5 - 20
maybe it needs to have a stack of frames so it knows where it's at? 
maybe we just dont have num_moves?

What do we need...
- rew (works for long sequences)
- gen_poss
- poss_this_turn

Things to try 
- can rew be guessed for shorter sequences if I stack frames? 
- can closeness be learned? (far frames vs close frames?)

you were using view instead of permute to change the color chanel in your frame preprocess
It wasn't just permuting the stuff, it was really changing it...

view != permute 

[(model(includes loss), inputs, labels, m?odel_x_optimizer?), (...)]

so the problem with this many models thing is that if im going to optimize with the 
jacobian I need to have each model output enough to optimize for it

in this case we have no nones
have 2 datasets (one for actor, one for planner)
train twice
one specializes in acting, the other in plannning.
imo they're both good.
this one doesn't need a lot of the stuff the othe one does.

Planner - obs, goal, noise(for when making a subplan)
- gen_poss
- midpoint
- rew
- num_moves

Actor - obs, goal, noise
- poss_this_turn
- rew
- num_moves
- acts


So what am I trying to do with these none values really...
The problem is that it needs to be done on a batch level...
So we have a network, and sometimes we know and sometimes we don't 
What would really solve this is to do the vision encoding seperate from the other stuff...
With this we would train the vision thing seperate and then train all the tails...
- I just like this much more honestly

What does it look like
- vision 2x -> rew, action.
   - trains on this problem
- visionvects -> acts
- visionvects -> rew
- vicionvects -> x 
- just do this for everything...
so the annoying part is that we already kinda have this, but if we want to do
- this is what we would need to do unfortuntalely
or we could make a key or something.....................!
- we would for each memory object have somethig keeping track of whether each thing was to be learned on
- When we batch we need to somehow keep this info
- then we need to push the outputs on to these labels
- if it's a good act its a good plans
- if it's a bad act its a bad plan
- if it's a good plan it's a bad act
- if it's a bad act its a good plan


The examples seem to be always in the same order...
So maybe it's not getting a truly random sample of the data?

optimizer 

gen_poss
poss_this_turn
midpoint
acts
num_moves
rew
